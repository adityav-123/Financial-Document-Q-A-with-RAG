{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8593dbd4",
   "metadata": {},
   "source": [
    "# üìÑ AI Financial Analyst: Document Q\\&A with RAG\n",
    "\n",
    "Natural Language Processing | üß† Retrieval-Augmented Generation | ü§ñ LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Notebook Summary\n",
    "\n",
    "This notebook implements a complete **Retrieval-Augmented Generation (RAG)** pipeline from scratch. The goal is to take a large, unstructured PDF document ‚Äî such as a company's 10-K annual report ‚Äî and turn it into a **searchable knowledge base**.\n",
    "\n",
    "We can then ask **natural language questions** and receive **accurate, source-backed answers** from a locally-run Large Language Model (LLM).\n",
    "\n",
    "The entire process, from **data extraction** to **final answer generation**, is self-contained within this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Pipeline Stages\n",
    "\n",
    "This notebook is divided into four main parts that execute the full RAG workflow:\n",
    "\n",
    "### 1. üßæ Text Extraction\n",
    "\n",
    "* **Goal**: Process the source PDF document\n",
    "* **Method**: Uses the `PyMuPDF` library to parse the PDF page by page and extract all its text content into a single string\n",
    "\n",
    "### 2. ‚úÇÔ∏è Text Chunking\n",
    "\n",
    "* **Goal**: Break down the massive text into smaller, meaningful pieces for analysis\n",
    "* **Method**: Employs LangChain's `RecursiveCharacterTextSplitter` to divide the text into overlapping chunks of a fixed size, ensuring context is preserved across splits\n",
    "\n",
    "### 3. üì¶ Vector Store Creation\n",
    "\n",
    "* **Goal**: Create a searchable \"knowledge base\" from the text chunks\n",
    "* **Method**:\n",
    "\n",
    "  * Uses the `sentence-transformers/all-MiniLM-L6-v2` model to convert each chunk into a numerical embedding\n",
    "  * Stores these embeddings in a **FAISS** index, a highly efficient similarity search library\n",
    "\n",
    "### 4. ‚ùì Q\\&A Inference\n",
    "\n",
    "* **Goal**: Ask a question and generate an answer based on the document\n",
    "* **Method**:\n",
    "\n",
    "  * The user's question is embedded using the same model\n",
    "  * FAISS retrieves the most relevant chunks from the vector store\n",
    "  * The question and retrieved chunks are passed as context to `google/flan-t5-base`, which generates the final answer\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bf0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5e297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"NASDAQ_AAPL_2024.pdf\" \n",
    "output_text_file = \"extracted_report_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37a030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Success! Extracted 121 pages.\n",
      "Full text saved to 'extracted_report_text.txt'\n",
      "\n",
      "--- PREVIEW OF EXTRACTED TEXT ---\n",
      "\n",
      "--- Page 1 ---\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September¬†28, 2024\n",
      "or\n",
      "‚òê TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† to ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†.\n",
      "Commission File Number: 001-36743\n",
      "Apple Inc.\n",
      "(Exact name of Registrant as specified in its charter)\n",
      "California\n",
      "94-2404110\n",
      "(State or other jurisdiction\n",
      "of incorporation or organization)\n",
      "(I.R.S. Employer Identification No.)\n",
      "One Apple Park Way\n",
      "Cupertino, California\n",
      "95014\n",
      "(Address of principal executive offices)\n",
      "(Zip Code)\n",
      "(408) 996-1010\n",
      "(Registrant‚Äôs telephone number, including area code)\n",
      "Securities registered pursuant to Section 12(b) of the Act:\n",
      "Title of each class\n",
      "Trading symbol(s)\n",
      "Name of each exchange on which registered\n",
      "Common Stock, $0.00001 par value per share\n",
      "AAPL\n",
      "The Nasdaq Stock Market LLC\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    \n",
    "    num_pages = len(doc)\n",
    "    \n",
    "    for page_num in range(num_pages):\n",
    "        page = doc.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "\n",
    "        full_text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "        full_text += page_text\n",
    "    doc.close()\n",
    "\n",
    "    with open(output_text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "        \n",
    "    print(f\" Success! Extracted {num_pages} pages.\")\n",
    "    print(f\"Full text saved to '{output_text_file}'\")\n",
    "    print(\"\\n--- PREVIEW OF EXTRACTED TEXT ---\")\n",
    "    print(full_text[:1000])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: The file '{pdf_path}' was not found.\")\n",
    "    print(\"Please make sure the PDF file is in the same directory as this script and the filename is correct.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045594de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_file = \"extracted_report_text.txt\"\n",
    "output_chunks_file = \"report_chunks.pkl\"\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c17b777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the extracted text file.\n",
      "\n",
      " Success! The document was split into 558 chunks.\n",
      "Chunks saved to 'report_chunks.pkl'\n",
      "\n",
      "--- PREVIEW OF FIRST CHUNK ---\n",
      "--- Page 1 ---\n",
      "UNITED STATES\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "Washington, D.C. 20549\n",
      "FORM 10-K\n",
      "(Mark One)\n",
      "‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the fiscal year ended September¬†28, 2024\n",
      "or\n",
      "‚òê TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "For the transition period from ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† to ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†.\n",
      "Commission File Number: 001-36743\n",
      "Apple Inc.\n",
      "(Exact name of Registrant as specified in its charter)\n",
      "California\n",
      "94-2404110\n",
      "(State or other jurisdiction\n",
      "of incorporation or organization)\n",
      "(I.R.S. Employer Identification No.)\n",
      "One Apple Park Way\n",
      "Cupertino, California\n",
      "95014\n",
      "(Address of principal executive offices)\n",
      "(Zip Code)\n",
      "(408) 996-1010\n",
      "(Registrant‚Äôs telephone number, including area code)\n",
      "Securities registered pursuant to Section 12(b) of the Act:\n",
      "Title of each class\n",
      "Trading symbol(s)\n",
      "Name of each exchange on which registered\n",
      "Common Stock, $0.00001 par value per share\n",
      "AAPL\n",
      "The Nasdaq Stock Market LLC\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(input_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "    print(\"Successfully loaded the extracted text file.\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "    with open(output_chunks_file, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "    print(f\"\\n Success! The document was split into {len(chunks)} chunks.\")\n",
    "    print(f\"Chunks saved to '{output_chunks_file}'\")\n",
    "    print(\"\\n--- PREVIEW OF FIRST CHUNK ---\")\n",
    "    print(chunks[0])\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: The file '{input_text_file}' was not found.\")\n",
    "    print(\"Please make sure you have successfully run the previous extraction step.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f48359",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chunks_file = \"report_chunks.pkl\"\n",
    "faiss_index_path = \"faiss_index\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcf202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 558 chunks from 'report_chunks.pkl'.\n",
      "Initializing embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADITYA VERMA\\AppData\\Local\\Temp\\ipykernel_21008\\3246803252.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADITYA VERMA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0823198632040bc9827974d61419a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADITYA VERMA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADITYA VERMA\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a7efa86c1b4f2c88a7b968ae3ad54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e248d0134d9f4f4bbb4f8ffd917eb3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49510a3b82394f8b8b1bb1874a78a985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4106ad392df84cff9c4b561ba4027e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aec11a47af45a3bc3a8954b4d6427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c4bb030ee34c7ba52c604e561f7365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf917fd637da43e99aabfd8eaff30881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dc653a941548b7a5c06c28677d5dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddcacad71814ef7b154895a6f308023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9369b45a71454ab8dfae934ae27fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from chunks. This may take a moment...\n",
      "\n",
      " Success! Vector store created and saved to 'faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(input_chunks_file, 'rb') as f:\n",
    "        chunks = pickle.load(f)\n",
    "    print(f\"Successfully loaded {len(chunks)} chunks from '{input_chunks_file}'.\")\n",
    "    print(f\"Initializing embedding model: {model_name}\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(\"Creating vector store from chunks. This may take a moment...\")\n",
    "\n",
    "    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)\n",
    "    vectorstore.save_local(faiss_index_path)\n",
    "    \n",
    "    print(f\"\\n Success! Vector store created and saved to '{faiss_index_path}'.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: The file '{input_chunks_file}' was not found.\")\n",
    "    print(\"Please make sure you have successfully run the previous chunking step.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fe4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index_path = \"faiss_index\"\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "llm_model_name = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2550bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Loading FAISS vector store...\n",
      "Vector store loaded successfully.\n",
      "Loading LLM: google/flan-t5-base. This might take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM pipeline created.\n",
      "Q&A chain is ready.\n",
      "\n",
      "Asking question: What were the main risks identified by the company?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANSWER ---\n",
      "design and manufacturing defects that could materially adversely affect the Company‚Äôs business and result in harm to the Company‚Äôs reputation ------------\n",
      "\n",
      "--- RELEVANT SOURCES ---\n",
      "Source 1 (from Page to):\n",
      "The Company‚Äôs operations are also subject to the risks of industrial accidents at its suppliers and contract manufacturers. While the Company‚Äôs suppliers are\n",
      "required to maintain safe working environments and operations, an industrial accident could occur and could result in serious injuries or loss...\n",
      "--------------------\n",
      "Source 2 (from Page affect):\n",
      "--- Page 12 ---\n",
      "The Company‚Äôs products and services may be affected from time to time by design and manufacturing defects that could materially adversely affect\n",
      "the Company‚Äôs business and result in harm to the Company‚Äôs reputation.\n",
      "The Company offers complex hardware and software products and servic...\n",
      "--------------------\n",
      "Source 3 (from Page periods.):\n",
      "Because of the following factors, as well as other factors affecting the Company‚Äôs results of operations and financial condition, past financial performance should\n",
      "not be considered to be a reliable indicator of future performance, and investors should not use historical trends to anticipate results...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Loading embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    print(\"Loading FAISS vector store...\")\n",
    "    vectorstore = FAISS.load_local(faiss_index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vector store loaded successfully.\")\n",
    "\n",
    "    print(f\"Loading LLM: {llm_model_name}. This might take a while...\")\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "    pipe = transformers.pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    print(\"LLM pipeline created.\")\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"refine\",  \n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    print(\"Q&A chain is ready.\")\n",
    "    question = \"What were the main risks identified by the company?\"\n",
    "    print(f\"\\nAsking question: {question}\")\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(\"\\n--- ANSWER ---\")\n",
    "    print(result['result'])\n",
    "    \n",
    "    print(\"\\n--- RELEVANT SOURCES ---\")\n",
    "    for i, source in enumerate(result['source_documents']):\n",
    "        try:\n",
    "            page_info = f\"(from Page {source.page_content.splitlines()[1].split()[-1]})\"\n",
    "        except IndexError:\n",
    "            page_info = \"(Page number not found)\"\n",
    "            \n",
    "        print(f\"Source {i+1} {page_info}:\")\n",
    "        print(source.page_content[:300] + \"...\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a7173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
